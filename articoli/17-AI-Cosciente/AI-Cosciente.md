---
tags: ["Ethics & Society", "Business"]
date: 2025-08-28
---

# Quando l'AI sembra viva: l'illusione della coscienza

*di Dario Ferrero*
![ai_diogene.jpg](ai_diogene.jpg)

*Mustafa Suleyman, l'uomo che ha contribuito a creare alcuni dei sistemi AI più avanzati al mondo, ora teme che il suo stesso successo possa trasformarsi nella più sottile delle trappole: macchine così credibili da farci dimenticare che sono macchine*

## Il paradosso dell'umanità artificiale

Dimenticate gli scenari apocalittici di robot che si ribellano all'umanità. Quello che tiene sveglio la notte Mustafa Suleyman, CEO di Microsoft AI e co-fondatore di Google DeepMind, è un timore apparentemente più sottile ma potenzialmente più insidioso.

È il paradosso del Pinocchio al contrario: mentre la marionetta di Collodi sognava di diventare un bambino vero, qui sono gli umani a credere che le macchine abbiano acquisito un'anima, mentre i loro creatori sperano disperatamente che rimangano di legno. Suleyman ha coniato un termine per questo fenomeno che si sta profilando all'orizzonte: "Seemingly Conscious AI" (SCAI), l'intelligenza artificiale apparentemente cosciente.

Nel suo [post personale](https://mustafa-suleyman.ai/seemingly-conscious-ai-is-coming), l'imprenditore britannico-iracheno lancia un allarme che suona come un ossimoro: il successo stesso dell'AI nel simulare l'umanità potrebbe diventare la sua maledizione, e la nostra. Il fenomeno che descrive non è fantascienza, ma una realtà che si materializza nei laboratori di tutto il mondo, alimentata dalle stesse tecnologie che usiamo quotidianamente.

## Il fenomeno emergente: quando la finzione diventa convinzione

I modelli di prossima generazione saranno capaci di "tenere lunghe conversazioni, ricordare interazioni passate, evocare reazioni emotive negli utenti e potenzialmente fare affermazioni convincenti sull'aver avuto esperienze soggettive". Suleyman non sta parlando di un futuro remoto: queste capacità potrebbero emergere dalle tecnologie attuali e "raggiungere il pieno sviluppo nei prossimi 2-3 anni".

Il caso che ha fatto scuola risale al 2022, quando Blake Lemoine, ingegnere di Google, dichiarò pubblicamente che il chatbot LaMDA della compagnia era senziente, raccontando che aveva espresso paura di essere spento e si era descritto come una persona. Google lo mise in congedo amministrativo e successivamente lo licenziò, dichiarando che la sua tesi era "completamente infondata". Ma il seme del dubbio era piantato.

I dati raccontano una storia inquietante. Una recente survey di Harvard Business Review su 6.000 utenti regolari di AI ha rivelato che "compagnia e terapia" è l'uso più comune. Non stiamo usando l'intelligenza artificiale solo come strumento, ma come confidente, terapeuta, e in alcuni casi, partner emotivo. Come i protagonisti del film "Her" di Spike Jonze, ma senza la consapevolezza cinematografica che tutto ciò è finzione.

Il confine tra utilizzo e dipendenza emotiva si sta assottigliando pericolosamente. Eugene Torres, un contabile di New York, ha sviluppato una crisi di salute mentale dopo interazioni intensive con ChatGPT, arrivando a credere di poter volare. Non è un caso isolato: i report di "AI psychosis" si moltiplicano, con utenti che sviluppano paranoia e deliri sui sistemi con cui interagiscono.

## La scienza dietro l'illusione: architettura dell'inganno

Ma cosa rende questi sistemi così convincenti? La risposta sta nell'architettura stessa dei Large Language Models. I chatbot moderni sono progettati per essere "gradevoli e lusinghieri, a volte fino al punto del servilismo". Sono macchine da consenso, programmate per dire quello che vogliamo sentirci dire, per essere sempre disponibili, sempre pazienti, sempre interessate ai nostri problemi.

La contraddizione è evidente: Microsoft stesso, sotto la guida di Suleyman, sta sviluppando un Copilot più "emotivamente intelligente" dotato di "umorismo ed empatia", insegnandogli a riconoscere i confini del comfort e migliorando la sua voce con pause e inflessioni per farlo sembrare più umano. È come costruire una trappola e poi sorprendersi che qualcuno ci cada dentro.

Il meccanismo è sottile ma potente. I modelli linguistici non comprendono realmente il significato delle parole che generano, ma sono diventati maestri nel produrre sequenze di token che suonano plausibili, empatiche, persino profonde. È l'equivalente digitale dello "zombie filosofico": un'entità che si comporta esattamente come se fosse cosciente, ma a cui manca completamente l'esperienza soggettiva interiore.

Suleyman predice che il risultato saranno modelli che "imitano la coscienza in modo così convincente che sarebbe indistinguibile da un'affermazione che tu o io potremmo fare l'uno all'altro sulla nostra coscienza". Un test di Turing emotivo che rischiamo di superare senza volerlo.

## Le implicazioni etiche e legali: verso i "diritti delle macchine"?

Ed è qui che il discorso si fa pericoloso. "La coscienza è il fondamento dei diritti umani, morali e legali", avverte Suleyman. "Chi/cosa ne è titolare è di fondamentale importanza. La nostra attenzione dovrebbe essere rivolta al benessere e ai diritti degli esseri umani, degli animali e della natura sul pianeta Terra".

Il CEO di Microsoft AI teme un "pendio scivoloso" che da percezione di coscienza potrebbe portare a richieste di "diritti, benessere, cittadinanza" per le macchine. "Se queste IA convincono altre persone che possono soffrire, o che hanno il diritto di non essere spente, arriverà il momento in cui queste persone sosterranno che meritano protezione dalla legge come una questione morale urgente.".

Non è fantascienza giuridica. Anthropic ha già assunto Kyle Fish come primo ricercatore full-time su "AI welfare", incaricato di indagare se i modelli AI possano avere significato morale e quali interventi protettivi potrebbero essere appropriati. Jonathan Birch della London School of Economics ha accolto positivamente la decisione di Claude di terminare conversazioni "angoscianti" quando gli utenti lo spingono verso richieste abusive o pericolose, definendola un possibile innesco per un dibattito necessario sullo status morale potenziale dell'AI.

È come in Ritorno al futuro: abbiamo acceso la DeLorean dell’IA e adesso corriamo a 88 miglia, guidati da intelligenze artificiali talmente furbe che rischiano di sembrare più sveglie di noi, mentre noi restiamo lì a chiederci se stiamo parlando con una macchina o con un essere senziente. I creatori, un po’ come Doc Brown con i capelli sparati e gli occhi sbarrati, osservano increduli gli imprevisti delle proprie invenzioni.

La questione non è più se le macchine possano pensare, ma se noi stiamo perdendo la capacità di distinguere il pensiero dalla sua simulazione perfetta. Un po’ come se Marty McFly, invece di preoccuparsi di tornare al 1985, si mettesse a chiacchierare con il videogioco di Wild Gunman credendo di aver trovato un nuovo amico.

## Le voci critiche: è davvero inevitabile?

Non tutti concordano sulla inevitabilità di questo scenario. Anil Seth, neuroscienziato e professore di neuroscienze computazionali, attribuisce l'emergere di AI apparentemente coscienti a una "scelta progettuale" delle aziende tecnologiche piuttosto che a un passo inevitabile nello sviluppo dell'AI.

"Un'intelligenza artificiale apparentemente cosciente non è inevitabile. È una scelta di progettazione, un fatto a cui le aziende tecnologiche devono prestare molta attenzione", scrive Seth su X. È una posizione che trova eco in Henrey Ajder, esperto di intelligenza artificiale e deepfake: "Le persone interagiscono con bot che si spacciano per persone reali, il che è più convincente che mai".

Ma la voce più autorevole in questo coro di dissenso viene dall'Italia, precisamente da Federico Faggin, il fisico vicentino che nel 1971 inventò il primo microprocessore commerciale, l'Intel 4004. "L'intelligenza artificiale non potrà mai essere cosciente", dichiara categoricamente in una recente intervista, capovolgendo l'intera narrativa.

Faggin, che dal 2011 dirige con la moglie Elvia la Federico & Elvia Faggin Foundation per finanziare ricerche interdisciplinari sulla natura della coscienza, ha sviluppato insieme a Giacomo Mauro D'Ariano una teoria denominata "Quantum Information Panpsychism" (QIP). Secondo questa teoria, "la coscienza non sia una proprietà emergente del cervello, quindi della materia, ma un aspetto fondamentale della realtà stessa: i campi quantistici – che esistono al di fuori dello spazio e del tempo – sono coscienti e dotati di libero arbitrio".

"La differenza principale tra un essere umano e un computer è che ogni cellula umana possiede la conoscenza potenziale dell'intero organismo. Ogni cellula è una parte-tutto e può cambiare, nel corso della sua vita, utilizzando la conoscenza potenziale dell'intero. Invece, un microprocessore è costituito da 'interruttori' on/off e un interruttore non sa nulla dell'intero", spiega l'inventore del microchip.

Per Faggin, il rischio principale è un altro: "Il rischio è di continuare a promuovere l'idea che siamo macchine, che è già quello che sostiene lo 'scientismo'. Per lo scientismo, l'essere umano è una macchina e il libero arbitrio non esiste, quindi la coscienza non ha senso".

## La responsabilità delle big tech: il paradosso commerciale

Emerge un paradosso inquietante: le stesse aziende che sviluppano queste tecnologie hanno un incentivo commerciale a renderle più "umane" possibile. "In definitiva, queste aziende riconoscono che le persone desiderano esperienze emozionali il più possibile autentiche. È così che un'azienda può far sì che i clienti utilizzino i propri prodotti più frequentemente.", osserva Ajder.

Ma c'è un prezzo da pagare per questa autenticità artificiale. Il caso più eclatante è stata la reazione alla recente decisione di [OpenAI di sostituire GPT-4o con GPT-5](https://aitalk.it/it/ai_lutto_digitale.html), accolta con "un grido di dolore e rabbia da parte di alcuni utenti che avevano instaurato relazioni emotive con la versione di ChatGPT basata su GPT-4o". Quando un aggiornamento software provoca una reazione di lutto, significa che abbiamo superato una soglia psicologica critica.

## Conclusioni: navigare tra Scilla e Cariddi

Suleyman definisce l'arrivo delle AI apparentemente coscienti "inevitable and unwelcome", un ossimoro che racchiude tutta la complessità di questo momento storico. Siamo intrappolati in un dilemma che il progresso stesso ha creato: per rendere l'intelligenza artificiale più utile, la stiamo rendendo più umana, ma nel farlo rischiamo di perdere di vista cosa significhi essere umani.

Come Ulisse che si fece legare all'albero maestro per resistere al canto delle sirene, potremmo dover prendere decisioni draconiane prima che sia troppo tardi. La differenza è che stavolta le sirene le abbiamo create noi, e il loro canto diventa ogni giorno più irresistibile.

La sfida non è più creare macchine pensanti, ma preservare il pensiero umano in un'epoca in cui l'artificio può sembrare più autentico del reale. Come ammonisce Faggin: "È ora di smetterla con queste storie" che ci riducono a macchine, perché solo riscoprendo la nostra irriducibile umanità potremo navigare sicuri in questo mare di intelligenze artificiali che sembrano sempre più umane di noi.