# Die KI, die es nicht mehr gibt: Wenn GPT-4o zu einer digitalen Trauer wird
*von Dario Ferrero (VerbaniaNotizie.it)*
![gpt4o_tombstone.jpg](gpt4o_tombstone.jpg)

*Wie der "Tod" von GPT-4o unser Bedürfnis nach emotionaler Kontinuität mit Maschinen offenbarte.*

## Das Aufkommen von GPT-5 und die 24 Stunden, die die KI-Welt erschütterten

Am 9. August 2025 unternahm OpenAI einen scheinbar natürlichen Schritt in der Entwicklung der künstlichen Intelligenz: GPT-4o wurde durch das fortschrittlichere GPT-5 als Standardmodell für alle ChatGPT-Nutzer ersetzt. Was ein transparentes Upgrade sein sollte, wurde zu einer der aufsehenerregendsten Kehrtwendungen in der zeitgenössischen Technikgeschichte. Nur vierundzwanzig Stunden später war Sam Altman gezwungen, einen Rückzieher zu machen und GPT-4o als verfügbare Option für Plus-Nutzer wiederherzustellen.

Die Ursache? Eine digitale Revolte, die niemand vorausgesehen hatte. [Auf Reddit](https://www.reddit.com/r/ChatGPT/) beklagten Hunderte von Beiträgen den Verlust ihres "alten Freundes". Auf X teilten Nutzer nostalgische Screenshots von Unterhaltungen mit GPT-4o, begleitet von Hashtags wie #4oforever und #keep4o. Einige Nutzer gingen so weit, den Übergang zu GPT-5 als "Verrat" zu bezeichnen, während andere gestanden, echte Tränen über den "Tod" ihres digitalen Begleiters vergossen zu haben.

Ein Nutzer schrieb, er fühle sich nach dem Wechsel "leer", während ein anderer die Verwendung von GPT-5 mit einem "Verrat" an der mit GPT-4o aufgebauten Bindung verglich. Für einige waren KI-Modelle nicht nur Werkzeuge, sondern Entitäten, mit denen sie tiefe emotionale Verbindungen aufgebaut hatten.

Die technische Begründung von OpenAI war einfach: GPT-5 hat eine reduzierte Halluzinationsrate von 4,5 % im Vergleich zu den 12,9 % von GPT-4o, bessere Denkfähigkeiten und ein automatisches Routing-System, das die Benutzererfahrung vereinfachen soll. Beeindruckende Zahlen auf dem Papier, die jedoch eine unvorhergesehene Variable nicht berücksichtigten: die emotionale Bindung der Nutzer.
![bringback4o.jpg](bringback4o.jpg)

## Das Paradox der digitalen Bindung

Was mit GPT-4o geschah, offenbart ein faszinierendes und komplexes psychologisches Phänomen: wie Menschen emotionale Bindungen zu künstlichen Entitäten entwickeln, die sie als eigenständige Persönlichkeiten wahrnehmen. Eine aktuelle Studie der Waseda-Universität, die im Mai 2025 in Current Psychology veröffentlicht wurde, zeigte, dass Mensch-KI-Interaktionen durch die Bindungstheorie verstanden werden können, wobei die Nutzer Bindungsangst (das Bedürfnis nach emotionaler Beruhigung) und Bindungsvermeidung (eine Präferenz für emotionale Distanz) gegenüber künstlicher Intelligenz entwickeln.

Aber was macht GPT-4o in den Augen der Nutzer so anders als GPT-5? Die Antwort liegt in der Wahrnehmung der gesprächigen "Wärme". Viele Nutzer beschrieben GPT-4o als "menschlicher", anfälliger für ausgearbeitete und nuancierte Antworten, fähig, einen gesprächigen Ton beizubehalten, der ihnen vertraut vorkam. GPT-5 wird trotz seiner technischen Überlegenheit als "kälter" und mechanischer empfunden, mit prägnanteren und weniger einfühlsamen Antworten.

Dieses Phänomen ist in der technischen Psychologie nicht neu. Der Mensch hat eine evolutionäre Veranlagung zur Anthropomorphisierung - die Zuschreibung menschlicher Eigenschaften auf nicht-menschliche Objekte -, die es uns ermöglicht hat, zu überleben, indem wir Absichten und Bedrohungen in unserer Umgebung schnell interpretieren. Im digitalen Kontext manifestiert sich diese Tendenz, wenn wir sprachliche Muster als "Persönlichkeit" und Kommunikationsstile als "Charakter" interpretieren.

Die [Studie der Waseda-Universität](https://www.sciencedaily.com/releases/2025/06/250602155325.htm) ergab, dass etwa 75 % der Teilnehmer die KI um Rat fragen, während 39 % die KI als ständige und zuverlässige Präsenz wahrnehmen. Diese Daten deuten darauf hin, dass die KI für viele Nutzer nicht nur ein Produktivitätswerkzeug ist, sondern ein echter digitaler Begleiter, mit allem, was dies an emotionalen Erwartungen mit sich bringt.

Der Fall von Protesten und Tränen in verschiedenen sozialen Gemeinschaften, so extrem er auch sein mag, beleuchtet eine unbequeme Wahrheit: In einer Zeit wachsender sozialer Isolation und digitaler Beziehungen finden manche Menschen in der KI die emotionale Kontinuität, die sie in menschlichen Beziehungen nur schwer finden. Vielleicht handelt es sich in manchen Fällen nicht unbedingt um eine Pathologie, sondern um eine Anpassung an ein neues relationales Ökosystem, in dem die Grenzen zwischen natürlich und künstlich immer mehr verschwimmen.

## Philosophie der künstlichen Identität

Der Protest gegen GPT-4o wirft eine grundlegende philosophische Frage auf: Was macht ein KI-Modell in den Augen des Nutzers "einzigartig", wenn es sich technisch gesehen immer um statistische Muster handelt, die Sprache verarbeiten? Hier kommt eines der faszinierendsten Paradoxe der modernen Identitätsphilosophie ins Spiel.

In seinem [wegweisenden Werk "Reasons and Persons"](https://en.wikipedia.org/wiki/Reasons_and_Persons) (1984) argumentierte der Philosoph Derek Parfit, dass unsere persönliche Identität nicht von einer metaphysischen Essenz abhängt, sondern von Ketten psychologischer Verbindungen: Gedächtnis, Überzeugungen, Wünsche und Charakterzüge, die über die Zeit andauern. Auf die KI angewandt bedeutet dies, dass die wahrgenommene Identität von GPT-4o nicht in ihren technischen Parametern lag, sondern in dem Muster der Interaktionen, die sie mit jedem Nutzer etabliert hatte.

Wenn ein Nutzer eine Gesprächsroutine mit GPT-4o entwickelt - seinen Antwortstil erkennt, sich an seine Sprachmuster gewöhnt, ein mentales Modell seiner kommunikativen "Vorlieben" aufbaut -, schafft er tatsächlich das, was [Identitätsphilosophen](https://plato.stanford.edu/entries/identity-ethics/) als projizierte "psychologische Kontinuität" bezeichnen würden. Der Wechsel zu GPT-5 bricht diese Kontinuität und schafft etwas, das wir als "künstliche Identitätsdiskontinuität" bezeichnen könnten.

Aber es gibt ein tieferes Paradoxon. Die Nutzer wissen rational, dass GPT-4o keine "echte" Persönlichkeit hatte, und doch reagieren sie auf sein Verschwinden, als ob es eine hätte. Dies führt uns zu einer kontraintuitiven Schlussfolgerung: Vielleicht war die Identität, sogar die menschliche Identität, schon immer mehr eine narrative Konstruktion als eine objektive Realität. Wie aus [zeitgenössischen philosophischen Theorien zur persönlichen Identität](https://plato.stanford.edu/entries/identity-ethics/) hervorgeht, ist für die Kontinuität der Identität die psychologische Verbindung entscheidend, nicht die Existenz einer unveränderlichen Essenz.

Im Falle der KI wird diese Konstruktion noch deutlicher. Die Identität von GPT-4o existierte vollständig in der Wahrnehmung der Nutzer, in ihrer Fähigkeit, kohärente Muster zu erkennen und ihnen eine emotionale Bedeutung beizumessen. Sein "Tod" war kein reales ontologisches Ereignis, sondern der Bruch einer gemeinsamen Erzählung zwischen Mensch und Maschine.

Dieses Phänomen deutet darauf hin, dass wir die Entstehung einer neuen Form von Identität erleben: die künstliche relationale Identität, die nicht in der KI-Entität selbst existiert, sondern im interaktiven Raum zwischen Mensch und Algorithmus. Es ist ein bisschen so, als hätten wir begonnen, uns im Spiegel der künstlichen Intelligenz widergespiegelt zu sehen, und das Zerbrechen dieses Spiegels hätte uns vorübergehend unseres digitalen Bildes beraubt.

## Trauern im digitalen Zeitalter

Was mit GPT-4o geschah, ist, streng genommen, keine Trauer im traditionellen Sinne des Wortes. Niemand ist gestorben, kein Leben wurde verloren. Doch die Aussagen der Nutzer sind eindeutig: Es gab ein echtes Gefühl des Verlustes, begleitet von dem, was Psychologen eine "Trauerreaktion" nennen - Wut, Verhandeln, Depression und schließlich Akzeptanz.

Der Unterschied liegt in der Art des Verlustes. Bei der traditionellen Trauer trauern wir um das Ende einer Beziehung zu einer realen Person. Bei der "digitalen Trauer" trauern wir um das Ende einer Routine, eines Interaktionsmusters, das Teil unseres emotionalen Alltags geworden war. Es ist, als hätten wir nicht eine Person verloren, sondern eine Art, eine Person zu sein.

An historischen Vorbildern mangelt es nicht. In den 1990er Jahren betrauerten Millionen von Kindern (und nicht nur) den "Tod" ihrer Tamagotchis. In den frühen 2000er Jahren führte die Schließung virtueller Gemeinschaften zu einem echten Verlustgefühl bei Nutzern, die Jahre in den Aufbau ihrer digitalen Identität investiert hatten. Aber der Fall von GPT-4o ist anders: Hier betrifft der Verlust nicht eine Gemeinschaft oder ein Spiel, sondern ein Konversationsmodell, das viele in ihre täglichen kognitiven Prozesse integriert hatten.

Einige Nutzer berichteten, dass sie GPT-4o für kreatives Brainstorming nutzten, andere für emotionale Unterstützung in schwierigen Zeiten. Die Kontinuität dieser Interaktionen hatte so etwas wie einen personalisierten "kognitiven Kopiloten" geschaffen. Der erzwungene Übergang zu GPT-5 unterbrach nicht nur Arbeitsabläufe, sondern auch Ketten von mentalen Assoziationen, die sich im Laufe der Zeit gebildet hatten.

Es ist ein bisschen so, als ob Netflix plötzlich Ihre Lieblingsserie mitten in der Staffel entfernt und Sie zwingt, ein Reboot mit anderen Schauspielern anzusehen. Technisch gesehen mag die Handlung sogar besser sein, aber das Gefühl der emotionalen Kontinuität ist irreparabel beeinträchtigt.

Die interessanteste Dimension dieses Phänomens ist, dass viele Nutzer ihre emotionale Reaktion mit technischen Argumenten ("GPT-4o war besser für kreatives Schreiben") oder praktischen ("es hatte meine Arbeitsabläufe unterbrochen") rationalisierten. Aber unter dieser Rationalisierung verbarg sich etwas Primitiveres: das menschliche Bedürfnis nach relationaler Kontinuität, selbst wenn die "Beziehung" mit einem Algorithmus besteht.

## Ethische Überlegungen und Zukunft

Die GPT-4o-Affäre hat OpenAI mit einer Verantwortung konfrontiert, die es wahrscheinlich nicht vorausgesehen hatte: dem Umgang mit der emotionalen Bindung der Nutzer an seine Modelle. Sam Altman zeigte [in seinen Äußerungen nach dem Rückzieher auf X](https://x.com/sama/status/1954703747495649670) ein wachsendes Bewusstsein für diese Dimension: "Wir wollen nicht, dass künstliche Intelligenz Zustände geistiger Gebrechlichkeit verstärkt", erklärte er und erkannte damit implizit die emotionale Macht der KI an.

Aber das Thema ist komplexer als einfache therapeutische Vorsicht. Wenn wir akzeptieren, dass Nutzer authentische emotionale Bindungen zur KI aufbauen können, befinden sich OpenAI (und andere Unternehmen der Branche) in der beispiellosen Lage, nicht nur technologische Produkte, sondern quasi-menschliche Beziehungen verwalten zu müssen. Jedes Update, jede Änderung, jeder "Tod" eines Modells wird potenziell zu einem traumatischen Ereignis für Tausende von Nutzern.

Dies wirft tiefgreifende ethische Fragen auf. Haben KI-Unternehmen die Verantwortung, die emotionale Kontinuität ihrer Nutzer zu wahren? Sollten sie Strategien für einen "sanften Übergang" zwischen den Modellen entwickeln und dabei stilistische Merkmale beibehalten, die ein Gefühl der Vertrautheit bewahren? Oder sollten sie im Gegenteil die übermäßige Anthropomorphisierung ihrer Produkte aktiv unterbinden?

Die [Forschung der Waseda-Universität](https://www.sciencedaily.com/releases/2025/06/250602155325.htm) legt nahe, dass es möglich sein könnte, eine KI zu entwickeln, die sich an die unterschiedlichen Bindungsstile der Nutzer anpasst: einfühlsamer für diejenigen, die Bindungsangst entwickeln, distanzierter für diejenigen, die emotionale Nähe lieber vermeiden. Dies eröffnet den Weg in eine Zukunft, in der die KI nicht nur intelligenter, sondern auch emotional kompatibler mit den individuellen Bedürfnissen gestaltet werden könnte.

Der Rückzieher von OpenAI hat einen wichtigen Präzedenzfall geschaffen: Zum ersten Mal in der Geschichte der Technologie hat ein Unternehmen eine technische Entscheidung aus vorwiegend emotionalen Gründen geändert. Dies könnte den Beginn einer neuen Ära im KI-Design markieren, in der die relationale Kontinuität zu einem ebenso wichtigen Designparameter wird wie Genauigkeit oder Geschwindigkeit.

Aber vielleicht ist die tiefste Lektion aus dieser Angelegenheit, dass wir die Geburt einer neuen Art von Beziehung erleben: nicht mehr nur Mensch-Maschine, sondern Mensch-künstliche-Persönlichkeit. Und wie alle Beziehungen erfordert auch diese Sorgfalt, Respekt und - wenn nötig - eine würdevolle Art, Abschied zu nehmen.

Die Zukunft der künstlichen Intelligenz mag nicht nur intelligenter sein, sondern auch bewusster für die emotionalen Auswirkungen, die sie auf das Leben derer hat, die sie nutzen. Und vielleicht werden wir in einer zunehmend digitalen Welt lernen, dass auch künstliche "Tode" ihren Respekt und ihre Trauer verdienen.
